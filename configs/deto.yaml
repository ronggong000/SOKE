NAME: DETO # Experiment names
ACCELERATOR: 'gpu' # Devices optioncal: “cpu”, “gpu”, “tpu”, “ipu”, “hpu”, “mps, “auto”
NUM_NODES: 1 # Number of GPU nodes for distributed training
DEVICE: [0,1,2,3,4,5] # Index of gpus eg. [0] or [0,1,2,3,4,5]

TRAIN:
  #---------------------------------
  STAGE: vae # stage "vae" , "lm_pretrain", "lm_instruct"
  #---------------------------------
  NUM_WORKERS: 16 # Number of workers
  BATCH_SIZE: 320 # Size of batches
  END_EPOCH: 500 # End epoch
  RESUME: '' # Resume training from this path
  PRETRAINED: '' # Preatrained model path

  OPTIM:
    target: AdamW
    params:
      lr: 2e-4
      betas: [0.9, 0.99]
      weight_decay: 0.0

  LR_SCHEDULER:
    target: CosineAnnealingLR
    params:
      T_max: ${TRAIN.END_EPOCH}  #${eval:${LOGGER.VAL_EVERY_STEPS} * 100}
      eta_min: 1e-6
    

# Evaluating Configuration
EVAL:
  BATCH_SIZE: 16 # Evaluating Batch size
  SPLIT: val

TEST:
  CHECKPOINTS: null
  SPLIT: test
  BATCH_SIZE: 16 # training Batch size
  REPLICATION_TIMES: 1 # Number of times to replicate the test
  SAVE_PREDICTIONS: False

DATASET:
  target: mGPT.data.H2S.H2SDataModule
  H2S:
    DATASET_NAME: how2sign_csl_phoenix
    ROOT: ../data/How2Sign
    CSL_ROOT: ../data/CSL-Daily
    PHOENIX_ROOT: ../data/Phoenix_2014T
    MEAN_PATH: ../data/CSL-Daily/mean.pt
    STD_PATH: ../data/CSL-Daily/std.pt
    MAX_MOTION_LEN: 400
    MIN_MOTION_LEN: 40
    MAX_TEXT_LEN: 20
    PICK_ONE_TEXT: true
    FRAME_RATE: 20.0
    UNIT_LEN: 4
    STD_TEXT: False

METRIC:
  # TYPE: ['TM2TMetrics', 'MRMetrics']
  TYPE: ['MRMetrics']

LOSS:
  LAMBDA_FEATURE: 1.0
  LAMBDA_VELOCITY: 0.0
  LAMBDA_COMMIT: 0.02
  LAMBDA_CLS: 1.0
  ABLATION:
    RECONS_LOSS: 'l1_smooth'

model:
  target: mGPT.models.mgpt.MotionGPT
  params:
    condition: 'text'
    task: 't2m'
    lm: ${lm.default}
    motion_vae: ${vq.re96}
    hand_vae_cfg: ${vq.hand192}
    rhand_vae_cfg: ${vq.hand192}


LOGGER:
  TYPE: ['wandb']
  VAL_EVERY_STEPS: 10  #10
  WANDB:
    params:
      project: SignGPT

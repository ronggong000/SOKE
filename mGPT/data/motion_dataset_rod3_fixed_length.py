import torch
import numpy as np
from torch.utils.data import DataLoader, Dataset
import os
import random
import json
class SignMotionFixedLengthDataset(Dataset):
    def __init__(self, data_dir, max_length, is_train=True,file_list=None,config=None):
        self.data_dir = data_dir
        self.max_length = max_length
        self.is_train = is_train
        self.config=config
        # 1. å°è¯•è¯»å–é¢„ç”Ÿæˆçš„å…ƒæ•°æ®ç¼“å­˜ (è§£å†³å¯åŠ¨æ…¢çš„é—®é¢˜)
        # å‡è®¾ä½ åœ¨ data_dir ä¸‹æ”¾äº†ä¸€ä¸ª metadata.jsonï¼Œé‡Œé¢å­˜äº†æ–‡ä»¶åå’Œé•¿åº¦
        cache_path = os.path.join(data_dir, "dataset_metadata.json")
        
        if os.path.exists(cache_path):
            print(f"Loading cached metadata from {cache_path}...")
            with open(cache_path, 'r') as f:
                metadata = json.load(f)
                # metadata ç»“æ„å»ºè®®: [{"name": "xxx.npz", "len": 100}, ...]
                # è¿‡æ»¤é€»è¾‘æ”¾åœ¨è¿™é‡Œ
                self.samples = []
                for item in metadata:
                    if item['len'] >= 8: # ä½ çš„è¿‡æ»¤æ¡ä»¶
                        # éªŒè¯/æµ‹è¯•é›†é€»è¾‘éœ€è¦åœ¨è¿™é‡Œå±•å¼€ï¼Œæˆ–è€…åªå­˜åŸºæœ¬ä¿¡æ¯ï¼Œgetitemé‡ŒåŠ¨æ€åˆ‡
                        # ä¸ºä¿æŒä¸ä½ åŸä»£ç é€»è¾‘ä¸€è‡´ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾ metadata å·²è¿‡æ»¤
                        if is_train:
                            self.samples.append(item['name'])
                            self.file_lengths = {x['name']: x['len'] for x in metadata}
                        else:
                            # éªŒè¯é›†åˆ‡ç‰‡é€»è¾‘ (ç¨å¾®å¤æ‚ç‚¹ï¼Œå¦‚æœ metadata åªå­˜äº†æ–‡ä»¶åå’Œæ€»é•¿)
                            T = item['len']
                            T = (T // 4) * 4 # å¯¹é½
                            for start_idx in range(0, T, max_length):
                                self.samples.append((item['name'], start_idx))
        else:
            print("âš ï¸ Cache not found! Scanning directory (this will be slow)...")
            # --- ä½ çš„åŸå§‹é€»è¾‘ (æ…¢) ---
            # å»ºè®®ï¼šç¬¬ä¸€æ¬¡è¿è¡Œå®Œè¿™ä¸ªæ…¢é€»è¾‘åï¼ŒæŠŠç»“æœä¿å­˜æˆ jsonï¼Œä¸‹æ¬¡å°±å¿«äº†
            self.data_files = [f for f in os.listdir(data_dir) if f.endswith('.npz')]
            self.file_lengths = {}
            self.samples = []
            
            temp_metadata = [] # ç”¨äºä¿å­˜ç¼“å­˜
            
            for filename in self.data_files:
                path = os.path.join(data_dir, filename)
                try:
                    # ä¼˜åŒ–ç‚¹ï¼šä½¿ç”¨ mmap_mode='r' è¯»å–å¤´éƒ¨ï¼Œä¸åŠ è½½æ•°æ®è¿›å†…å­˜ï¼Œé€Ÿåº¦å¿«å¾ˆå¤š
                    with np.load(path, mmap_mode='r') as data:
                        # åªæ˜¯è¯»å– shapeï¼Œéå¸¸å¿«
                        if self.config.xyz==True:
                            shape = data['joints_xyz'].shape 
                        else:
                            shape = data['poses'].shape 
                        T = shape[0]
                        
                        # è¿‡æ»¤é€»è¾‘
                        T_aligned = (T // 4) * 4
                        
                        if T_aligned >= 8:
                            self.file_lengths[filename] = T_aligned
                            temp_metadata.append({"name": filename, "len": T_aligned})
                            
                            if is_train:
                                self.samples.append(filename)
                            else:
                                for start_idx in range(0, T_aligned, max_length):
                                    self.samples.append((filename, start_idx))
                except Exception as e:
                    pass
            
            # è‡ªåŠ¨ä¿å­˜ç¼“å­˜ï¼Œä¸‹æ¬¡å°±ä¸ç”¨ç­‰äº†
            try:
                with open(cache_path, 'w') as f:
                    json.dump(temp_metadata, f)
                print(f"âœ… Created metadata cache at {cache_path}")
            except:
                pass

        print(f"Loaded {len(self.samples)} samples.")
    def calculate_stats(self):
        """
        è®¡ç®—ç»Ÿè®¡é‡ï¼š
        - XYZ æ¨¡å¼ï¼šé€ç»´ mean/std
        - ROT(è½´è§’) æ¨¡å¼ï¼šmean å¼ºåˆ¶ 0ï¼Œæ›´ç¨³çš„ std æ–¹æ¡ˆï¼š
            rot_norm = "none"      -> std=1 ä¸ç¼©æ”¾ï¼ˆæ¨èå…ˆè¯•ï¼Œæœ€ä¸å®¹æ˜“ä¼¤æ¨¡å‹ï¼‰
            rot_norm = "std_dim"   -> æ¯ä¸ªç»´åº¦çœŸå® stdï¼ˆæ¨èï¼‰
            rot_norm = "std_joint" -> æ¯ä¸ªå…³èŠ‚(3ç»´)ç”¨åŒä¸€ä¸ª stdï¼ˆæ›´ä¿å®ˆï¼‰
        """
        print(f"ğŸ“Š Calculating stats (XYZ mode: {self.config.xyz})...")
        all_data = []
        files_to_scan = self.samples if self.is_train else []

        for filename in files_to_scan:
            filepath = os.path.join(self.data_dir, filename)
            with np.load(filepath) as data:
                feat = data['joints_xyz'] if self.config.xyz else data['poses']
                feat = feat[:, self.config.SELECTED_JOINT_INDICES, :]  # [T, J, 3]
                all_data.append(feat.reshape(-1, feat.shape[1] * 3))   # [T, J*3]

        if len(all_data) == 0:
            raise RuntimeError("No training files found to calculate stats.")

        all_data = np.concatenate(all_data, axis=0).astype(np.float64)  # [Total_Frames, D]

        eps = 1e-5

        if self.config.xyz:
            mean = np.mean(all_data, axis=0)
            std = np.std(all_data, axis=0)
            std[std < eps] = 1.0
            return torch.from_numpy(mean).float(), torch.from_numpy(std).float()

        # ---------------- ROT mode (axis-angle) ----------------
        # mean å¿…é¡»ä¿æŒ 0ï¼ˆä¸ç„¶â€œæ— æ—‹è½¬â€çŠ¶æ€è¢«å¹³ç§»ï¼‰
        mean = np.zeros(all_data.shape[1], dtype=np.float64)

        # é€‰æ‹© rot å½’ä¸€åŒ–ç­–ç•¥ï¼šå°½é‡ä¸è¦æ±‚ä½ æ”¹ configï¼Œæ²¡æœ‰å°±èµ°é»˜è®¤
        rot_norm = getattr(self.config, "rot_norm", "none")
        # å¯é€‰ï¼š"none" / "std_dim" / "std_joint"

        if rot_norm == "none":
            std = np.ones(all_data.shape[1], dtype=np.float64)

        elif rot_norm == "std_joint":
            # æ¯ä¸ªå…³èŠ‚ 3 ç»´ç”¨ä¸€ä¸ª stdï¼ˆç”¨çœŸæ­£ stdï¼Œä¸ç”¨ RMSï¼‰
            std = np.ones(all_data.shape[1], dtype=np.float64)
            for j in range(0, all_data.shape[1], 3):
                joint = all_data[:, j:j+3]                 # [N, 3]
                joint_std = np.std(joint)                  # æ ‡é‡ï¼šå¯¹è¿™ 3 ç»´æ•´ä½“çš„çœŸå® std
                if joint_std < eps:
                    joint_std = 1.0
                std[j:j+3] = joint_std

        else:
            # "std_dim"ï¼šé€ç»´çœŸå® stdï¼ˆé€šå¸¸æœ€å¥½ï¼‰
            std = np.std(all_data, axis=0)
            std[std < eps] = 1.0

        return torch.from_numpy(mean).float(), torch.from_numpy(std).float()
    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        if self.is_train:
            filename = self.samples[idx]
            T = self.file_lengths[filename]
            if T > self.max_length:
                start_idx = random.randint(0, T - self.max_length)
            else:
                start_idx = 0
        else:
            filename, start_idx = self.samples[idx]

        filepath = os.path.join(self.data_dir, filename)
        # 2. ä¼˜åŒ– Runtime è¯»å–ï¼šä½¿ç”¨ mmap_mode='r'
        # è¿™æ ·å½“ä½ åšåˆ‡ç‰‡æ—¶ï¼Œåªä¼šä»ç¡¬ç›˜è¯»å–ä½ éœ€è¦çš„é‚£ä¸€å°å—æ•°æ®ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ–‡ä»¶
        # è¿™èƒ½æ˜¾è‘—é™ä½å†…å­˜å³°å€¼ (Memory Spike)
        try:
            with np.load(filepath, mmap_mode='r') as data:
                # æ³¨æ„ï¼šmmap è¿”å›çš„æ˜¯ç£ç›˜ä¸Šçš„è§†å›¾
                # data['poses'] æ­¤æ—¶æ²¡æœ‰è¯»å…¥å†…å­˜
                
                # è¿™ä¸€æ­¥åˆ‡ç‰‡æ“ä½œä¼šè§¦å‘å®é™…çš„ç£ç›˜è¯»å–ï¼Œä½†åªè¯»è¿™ä¸€å°å—
                # å¿…é¡»åŠ ä¸Š .copy() æˆ–è€… np.array() æŠŠå®ƒçœŸæ­£å˜æˆå†…å­˜é‡Œçš„ arrayï¼Œå¦åˆ™è½¬ Tensor ä¼šæŠ¥é”™
                if hasattr(self.config, 'xyz') and self.config.xyz==True:
                    full_motion_slice = data['joints_xyz'][start_idx : start_idx + self.max_length, self.config.SELECTED_JOINT_INDICES, :]
                else:
                    full_motion_slice = data['poses'][start_idx : start_idx + self.max_length, self.config.SELECTED_JOINT_INDICES, :]
                
                # è¿™é‡Œçš„åˆ‡ç‰‡é€»è¾‘éœ€è¦ç¨å¾®è°ƒæ•´ï¼Œå› ä¸ºæˆ‘ä»¬ä¸èƒ½å…ˆè¯» full å†åˆ‡ï¼Œé‚£æ ·å°±å¤±å» mmap çš„æ„ä¹‰äº†
                # ç°åœ¨çš„é€»è¾‘ï¼šç›´æ¥è¯»å–éœ€è¦çš„ time slice å’Œ joint slice
                
                # ä¸ºäº†å®‰å…¨ï¼Œå…ˆè¯»å–åˆ° numpy (è¿™å°±è¿›å…¥å†…å­˜äº†ï¼Œä½†åªæœ‰ä¸€å°å—)
                motion_data = np.array(full_motion_slice) 
                
        except Exception as e:
            # å®¹é”™å¤„ç†ï¼šè¿”å›å…¨0æˆ–è€…éšæœºæ•°æ®é˜²æ­¢ crash
            print(f"Error loading {filename}: {e}")
            return torch.zeros(self.max_length, len(self.config.SELECTED_JOINT_INDICES)*3), torch.tensor(0)

        # 3. åå¤„ç† (Padding ç­‰)
        # å› ä¸ºæˆ‘ä»¬ä¸Šé¢æ˜¯ç›´æ¥æŒ‰ start_idx + max_length åˆ‡çš„ï¼Œå¯èƒ½åˆ‡å‡ºæ¥çš„é•¿åº¦ä¸å¤Ÿ
        original_len = motion_data.shape[0]
        
        if original_len < self.max_length:
            pad_len = self.max_length - original_len
            last_frame = motion_data[-1:]
            padding = np.repeat(last_frame, pad_len, axis=0)
            motion_data = np.concatenate([motion_data, padding], axis=0)

        # Flatten
        motion_flat = motion_data.reshape(self.max_length, -1).astype(np.float32)
            
        return torch.from_numpy(motion_flat), torch.tensor(original_len)

def simple_collate_fn(batch):
    # batch ç°åœ¨æ˜¯ä¸€ä¸ªå…ƒç»„åˆ—è¡¨: [(motion1, len1), (motion2, len2), ...]
    motions, lengths = zip(*batch)
    
    # motions æ˜¯ä¸€ä¸ªå¼ é‡å…ƒç»„ï¼Œlengths æ˜¯ä¸€ä¸ªå¼ é‡å…ƒç»„
    # å°†å®ƒä»¬åˆ†åˆ«å †å æˆä¸€ä¸ªå¤§çš„æ‰¹æ¬¡å¼ é‡
    stacked_motions = torch.stack(motions, dim=0)
    stacked_lengths = torch.stack(lengths, dim=0)
    
    # è¿”å›ä¸¤ä¸ªå¼ é‡ï¼šä¸€ä¸ªæ˜¯æ‰¹æ¬¡æ•°æ®ï¼Œå¦ä¸€ä¸ªæ˜¯å¯¹åº”çš„é•¿åº¦
    return stacked_motions, stacked_lengths

def create_data_loaders(train_data_dir, val_data_dir, test_data_dir, batch_size, config=None):
    """
    ä»ä¸‰ä¸ªç‹¬ç«‹çš„æ–‡ä»¶å¤¹åˆ›å»ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•çš„ DataLoaderã€‚
    """
    num_workers = config.num_workers
    max_length = config.max_length

    # 1. ä¸ºè®­ç»ƒé›†åˆ›å»º Dataset å®ä¾‹
    # å®ƒä¼šè‡ªåŠ¨æ‰«æ train_data_dir æ–‡ä»¶å¤¹
    train_dataset = SignMotionFixedLengthDataset(
        data_dir=train_data_dir, 
        max_length=max_length, 
        is_train=True, 
        file_list=None, # è®¾ä¸º None, è®© Dataset è‡ªå·±æ‰«æ
        config=config
    )
    
    # 2. ä¸ºéªŒè¯é›†åˆ›å»º Dataset å®ä¾‹
    # å®ƒä¼šè‡ªåŠ¨æ‰«æ val_data_dir æ–‡ä»¶å¤¹
    val_dataset = SignMotionFixedLengthDataset(
        data_dir=val_data_dir, 
        max_length=max_length, 
        is_train=False, 
        file_list=None, # è®¾ä¸º None, è®© Dataset è‡ªå·±æ‰«æ
        config=config
    )

    # 3. ä¸ºæµ‹è¯•é›†åˆ›å»º Dataset å®ä¾‹
    # å®ƒä¼šè‡ªåŠ¨æ‰«æ test_data_dir æ–‡ä»¶å¤¹
    test_dataset = SignMotionFixedLengthDataset(
        data_dir=test_data_dir, 
        max_length=max_length, 
        is_train=False, # æµ‹è¯•é›†ä¸éªŒè¯é›†ä¸€æ ·ï¼Œis_train=False
        file_list=None, # è®¾ä¸º None, è®© Dataset è‡ªå·±æ‰«æ
        config=config
    )

    # 4. åˆ›å»º DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True, # è®­ç»ƒé›†éœ€è¦æ‰“ä¹±
        num_workers=num_workers,
        collate_fn=simple_collate_fn,
        pin_memory=True,
        drop_last=True,
        persistent_workers=True if num_workers > 0 else False,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False, # éªŒè¯é›†ä¸éœ€è¦æ‰“ä¹±
        num_workers=num_workers,
        collate_fn=simple_collate_fn,
        pin_memory=True,
        drop_last=True,
        persistent_workers=True if num_workers > 0 else False,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False, # æµ‹è¯•é›†ä¸éœ€è¦æ‰“ä¹±
        num_workers=num_workers,
        collate_fn=simple_collate_fn,
        pin_memory=True,
        drop_last=True, # ä½ å¯ä»¥æ ¹æ®éœ€è¦å†³å®šæµ‹è¯•é›†æ˜¯å¦ drop_last
        persistent_workers=True if num_workers > 0 else False,
    )
    
    print(f"Data loaders created.")
    return train_loader, val_loader, test_loader